{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__: 2.4.1\n",
      "keras.__version__: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# import cv2\n",
    "# import os\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "# from tensorflow.keras import Model\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# python standard libraries\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import random\n",
    "import fnmatch\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "# data processing\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':lambda x: \"%.4f\" % x})\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 300)\n",
    "pd.set_option('display.float_format', '{:,.4f}'.format)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential  # V2 is tensorflow.keras.xxxx, V1 is keras.xxx\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "print( f'tf.__version__: {tf.__version__}' )\n",
    "print( f'keras.__version__: {keras.__version__}' )\n",
    "\n",
    "# sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# imaging\n",
    "import cv2\n",
    "from imgaug import augmenters as img_aug\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from PIL import Image, ImageEnhance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "x_names = []\n",
    "\n",
    "with open('training_norm.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader)\n",
    "#     print(reader)\n",
    "    with open('training_norm_new.csv', mode='w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "#         c = 0\n",
    "        for rows in reader:\n",
    "#             if c > 100:\n",
    "#                 break\n",
    "            f = rows[0]\n",
    "            a = float(rows[1])\n",
    "            s = float(rows[2])\n",
    "            y.append([a,s])\n",
    "            filename = str(f) + '.png'\n",
    "            x_names.append(filename)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data_generator(image_paths, steering_angles, batch_size, is_training):\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_steering_angles = []\n",
    "        path = '/Users/Kamyab/Documents/UoN/MLiS_2/Project/Data/training_data/training_data/'\n",
    "       \n",
    "        for i in range(batch_size):\n",
    "            random_index = random.randint(0, len(image_paths) - 1)\n",
    "           \n",
    "            image_path = path + image_paths[random_index]\n",
    "            image = cv2.imread(image_path)\n",
    "            steering_angle = steering_angles[random_index]\n",
    "            if is_training:\n",
    "                # training: augment image\n",
    "                image, steering_angle = random_augment(image, steering_angle)\n",
    "             \n",
    "            image = img_preprocess(image)\n",
    "            batch_images.append(image)\n",
    "            batch_steering_angles.append(steering_angle)\n",
    "           \n",
    "        yield( np.asarray(batch_images), np.asarray(batch_steering_angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 11034\n",
      "Validation data: 2759\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split( x_names, y, test_size=0.2)\n",
    "print(\"Training data: %d\\nValidation data: %d\" % (len(X_train), len(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(image):\n",
    "    zoom = img_aug.Affine(scale=(1, 1.3))  # zoom from 100% (no zoom) to 130%\n",
    "    image = zoom.augment_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pan(image):\n",
    "    # pan left / right / up / down about 10%\n",
    "    pan = img_aug.Affine(translate_percent= {\"x\" : (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n",
    "    image = pan.augment_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_brightness(image):\n",
    "    # increase or decrease brightness by 30%\n",
    "    brightness = img_aug.Multiply((0.7, 1.3))\n",
    "    image = brightness.augment_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur(image):\n",
    "    kernel_size = random.randint(1, 5)  # kernel larger than 5 would make the image way too blurry\n",
    "    image = cv2.blur(image,(kernel_size, kernel_size))\n",
    "   \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip(image, steering_angle):\n",
    "    is_flip = random.randint(0, 1)\n",
    "    if is_flip == 1:\n",
    "        # randomly flip horizon\n",
    "        image = cv2.flip(image,1)\n",
    "        steering_angle[0] = (80 - steering_angle[0]*80)/80\n",
    "        #(180 - (steering_angle[0]*80 + 50) -50)/80\n",
    "   \n",
    "    return image, steering_angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_augment(image, steering_angle):\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = pan(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = zoom(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = blur(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = adjust_brightness(image)\n",
    "    image, steering_angle = random_flip(image, steering_angle)\n",
    "    \n",
    "    return image, steering_angle\n",
    "\n",
    "\n",
    "\n",
    "# show a few randomly augmented images\n",
    "# ncol = 2\n",
    "# nrow = 10\n",
    "# fig, axes = plt.subplots(nrow, ncol, figsize=(15, 50))\n",
    "\n",
    "# for i in range(nrow):\n",
    "#     rand_index = random.randint(0, len(image_paths) - 1)\n",
    "#     image_path = image_paths[rand_index]\n",
    "#     steering_angle_orig = steering_angles[rand_index]\n",
    "    \n",
    "#     image_orig = my_imread(image_path)\n",
    "#     image_aug, steering_angle_aug = random_augment(image_orig, steering_angle_orig)\n",
    "    \n",
    "#     axes[i][0].imshow(image_orig)\n",
    "#     axes[i][0].set_title(\"original, angle=%s\" % steering_angle_orig)\n",
    "#     axes[i][1].imshow(image_aug)\n",
    "#     axes[i][1].set_title(\"augmented, angle=%s\" % steering_angle_aug)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocess(image):\n",
    "    height, _, _ = image.shape\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)  # Nvidia model said it is best to use YUV color space\n",
    "    image = cv2.GaussianBlur(image, (3,3), 0)\n",
    "    image = cv2.resize(image, (160,120)) # input image size (200,66) Nvidia model\n",
    "    image = image / 255 # normalizing\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Nvidia_Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 31, 98, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 47, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 5, 22, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 3, 20, 64)         27712     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 18, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 252,230\n",
      "Trainable params: 252,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# def nvidia_model():\n",
    "#     model = Sequential(name='Nvidia_Model')\n",
    "    \n",
    "#     # elu=Expenential Linear Unit, similar to leaky Relu\n",
    "#     # skipping 1st hiddel layer (nomralization layer), as we have normalized the data\n",
    "    \n",
    "#     # Convolution Layers\n",
    "#     model.add(Conv2D(24, (5, 5), strides=(2, 2), input_shape=(66, 200, 3), activation='elu')) \n",
    "#     model.add(Conv2D(36, (5, 5), strides=(2, 2), activation='elu')) \n",
    "#     model.add(Conv2D(48, (5, 5), strides=(2, 2), activation='elu')) \n",
    "#     model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "#     model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
    "#     model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "    \n",
    "#     # Fully Connected Layers\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
    "#     model.add(Dense(100, activation='elu'))\n",
    "#     model.add(Dense(50, activation='elu'))\n",
    "#     model.add(Dense(10, activation='elu'))\n",
    "#     model.add(Dense(2, activation='tanh')) \n",
    "#     model.add(Dense(2, activation='sigmoid')) \n",
    "\n",
    "    \n",
    "#     # output layer: turn angle (from 45-135, 90 is straight, <90 turn left, >90 turn right)\n",
    "#     model.add(Dense(1))\n",
    "#     model.add(Dense(2))\n",
    "    \n",
    "#     # since this is a regression problem not classification problem,\n",
    "#     # we use MSE (Mean Squared Error) as loss function\n",
    "#     optimizer = Adam(lr=1e-3) # lr is learning rate\n",
    "#     model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = nvidia_model()\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Nvidia_Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 58, 78, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 27, 37, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 17, 48)        43248     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 15, 64)        27712     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 13, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6656)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6656)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               665700    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 802,630\n",
      "Trainable params: 802,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def nvidia_model():\n",
    "    model = Sequential(name='Nvidia_Model')\n",
    "    \n",
    "    # elu=Expenential Linear Unit, similar to leaky Relu\n",
    "    # skipping 1st hiddel layer (nomralization layer), as we have normalized the data\n",
    "    \n",
    "    # Convolution Layers\n",
    "    model.add(Conv2D(24, (5, 5), strides=(2, 2), input_shape=(120, 160, 3), activation='elu')) \n",
    "    model.add(Conv2D(36, (5, 5), strides=(2, 2), activation='elu')) \n",
    "    model.add(Conv2D(48, (5, 5), strides=(2, 2), activation='elu')) \n",
    "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    \n",
    "    # output layer: turn angle (from 45-135, 90 is straight, <90 turn left, >90 turn right)\n",
    "    model.add(Dense(2, activation='sigmoid'))#, activation='tanh')) \n",
    "    \n",
    "    # since this is a regression problem not classification problem,\n",
    "    # we use MSE (Mean Squared Error) as loss function\n",
    "    optimizer = Adam(lr=1e-3) # lr is learning rate\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    return model\n",
    "\n",
    " \n",
    "\n",
    "model = nvidia_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "300/300 [==============================] - 323s 1s/step - loss: 0.1198 - accuracy: 0.7411 - val_loss: 0.0555 - val_accuracy: 0.9116\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05549, saving model to /Users/Kamyab/Documents/lane_navigation_check.h5\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 327s 1s/step - loss: 0.0564 - accuracy: 0.9087 - val_loss: 0.0442 - val_accuracy: 0.9362\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05549 to 0.04417, saving model to /Users/Kamyab/Documents/lane_navigation_check.h5\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 338s 1s/step - loss: 0.0476 - accuracy: 0.9252 - val_loss: 0.0413 - val_accuracy: 0.9427\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04417 to 0.04134, saving model to /Users/Kamyab/Documents/lane_navigation_check.h5\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 340s 1s/step - loss: 0.0433 - accuracy: 0.9325 - val_loss: 0.0333 - val_accuracy: 0.9514\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04134 to 0.03333, saving model to /Users/Kamyab/Documents/lane_navigation_check.h5\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 347s 1s/step - loss: 0.0407 - accuracy: 0.9389 - val_loss: 0.0342 - val_accuracy: 0.9616\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03333\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 339s 1s/step - loss: 0.0390 - accuracy: 0.9431 - val_loss: 0.0311 - val_accuracy: 0.9603\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03333 to 0.03107, saving model to /Users/Kamyab/Documents/lane_navigation_check.h5\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 341s 1s/step - loss: 0.0357 - accuracy: 0.9515 - val_loss: 0.0301 - val_accuracy: 0.9632\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03107 to 0.03008, saving model to /Users/Kamyab/Documents/lane_navigation_check.h5\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 361s 1s/step - loss: 0.0336 - accuracy: 0.9580 - val_loss: 0.0289 - val_accuracy: 0.9671\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03008 to 0.02889, saving model to /Users/Kamyab/Documents/lane_navigation_check.h5\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 354s 1s/step - loss: 0.0347 - accuracy: 0.9552 - val_loss: 0.0291 - val_accuracy: 0.9707\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02889\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 340s 1s/step - loss: 0.0317 - accuracy: 0.9630 - val_loss: 0.0299 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02889\n",
      "Epoch 11/20\n",
      "300/300 [==============================] - 331s 1s/step - loss: 0.0313 - accuracy: 0.9637 - val_loss: 0.0261 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02889 to 0.02614, saving model to /Users/Kamyab/Documents/lane_navigation_check.h5\n",
      "Epoch 12/20\n",
      "300/300 [==============================] - 321s 1s/step - loss: 0.0304 - accuracy: 0.9647 - val_loss: 0.0307 - val_accuracy: 0.9607\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02614\n",
      "Epoch 13/20\n",
      "300/300 [==============================] - 333s 1s/step - loss: 0.0310 - accuracy: 0.9635 - val_loss: 0.0271 - val_accuracy: 0.9704\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02614\n",
      "Epoch 14/20\n",
      "300/300 [==============================] - 324s 1s/step - loss: 0.0299 - accuracy: 0.9657 - val_loss: 0.0281 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02614\n",
      "Epoch 15/20\n",
      "300/300 [==============================] - 323s 1s/step - loss: 0.0291 - accuracy: 0.9678 - val_loss: 0.0268 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02614\n",
      "Epoch 16/20\n",
      "300/300 [==============================] - 327s 1s/step - loss: 0.0293 - accuracy: 0.9685 - val_loss: 0.0261 - val_accuracy: 0.9746\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02614 to 0.02611, saving model to /Users/Kamyab/Documents/lane_navigation_check.h5\n",
      "Epoch 17/20\n",
      "300/300 [==============================] - 361s 1s/step - loss: 0.0290 - accuracy: 0.9699 - val_loss: 0.0280 - val_accuracy: 0.9784\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02611\n",
      "Epoch 18/20\n",
      "300/300 [==============================] - 335s 1s/step - loss: 0.0282 - accuracy: 0.9707 - val_loss: 0.0269 - val_accuracy: 0.9748\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02611\n",
      "Epoch 19/20\n",
      "300/300 [==============================] - 341s 1s/step - loss: 0.0290 - accuracy: 0.9689 - val_loss: 0.0296 - val_accuracy: 0.9726\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02611\n",
      "Epoch 20/20\n",
      "300/300 [==============================] - 352s 1s/step - loss: 0.0290 - accuracy: 0.9685 - val_loss: 0.0262 - val_accuracy: 0.9797\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02611\n"
     ]
    }
   ],
   "source": [
    "# saves the model weights after each epoch if the validation loss decreased\n",
    "model_output_dir = '/Users/Kamyab/Documents/'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(model_output_dir,'lane_navigation_check.h5'), verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit_generator(image_data_generator( X_train, y_train, batch_size=100, is_training=True),\n",
    "                              steps_per_epoch=300,\n",
    "                              epochs=20,\n",
    "                              validation_data = image_data_generator( X_valid, y_valid, batch_size=100, is_training=False),\n",
    "                              validation_steps=200,\n",
    "                              verbose=1,\n",
    "                              shuffle=1,\n",
    "                              callbacks=[checkpoint_callback])\n",
    "# always save model output as soon as model finishes training\n",
    "model.save(os.path.join(model_output_dir,'lane_navigation_final.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(model_output_dir,'lane_navigation_final.h5'))\n",
    "       \n",
    "def compute_steering_angle(self, frame):\n",
    "    preprocessed = img_preprocess(frame)\n",
    "    X = np.asarray([preprocessed])\n",
    "    steering_angle = model.predict(X)[0]\n",
    "    return steering_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.4546135 0.0\n",
      "2 0.5017879 1.0\n",
      "3 0.4972028 1.0\n",
      "4 0.49526978 1.0\n",
      "5 0.49662477 1.0\n",
      "6 0.51107186 1.0\n",
      "7 0.5026907 1.0\n",
      "8 0.49875063 1.0\n",
      "9 0.4439684 0.0\n",
      "10 0.49509922 1.0\n",
      "11 0.45275775 0.0\n",
      "12 0.46580517 0.0\n",
      "13 0.49763197 1.0\n",
      "14 0.44541043 0.0\n",
      "15 0.4971731 1.0\n",
      "16 0.49449804 1.0\n",
      "17 0.45530555 0.0\n",
      "18 0.45168072 0.0\n",
      "19 0.45175675 0.0\n",
      "20 0.4961738 1.0\n",
      "21 0.49839392 1.0\n",
      "22 0.49551728 1.0\n",
      "23 0.49648896 1.0\n",
      "24 0.4461876 0.0\n",
      "25 0.4689473 0.0\n",
      "26 0.50533354 1.0\n",
      "27 0.4794948 0.0\n",
      "28 0.4481284 0.0\n",
      "29 0.44277114 0.0\n",
      "30 0.5004794 1.0\n",
      "31 0.4965739 1.0\n",
      "32 0.4989713 1.0\n",
      "33 0.4510816 0.0\n",
      "34 0.49946368 1.0\n",
      "35 0.45763054 0.0\n",
      "36 0.50026643 1.0\n",
      "37 0.50053716 1.0\n",
      "38 0.50061786 1.0\n",
      "39 0.44431004 0.0\n",
      "40 0.50112647 1.0\n",
      "41 0.49552935 1.0\n",
      "42 0.49361184 1.0\n",
      "43 0.49599093 1.0\n",
      "44 0.46810782 0.0\n",
      "45 0.49761742 1.0\n",
      "46 0.4695144 0.0\n",
      "47 0.45756847 0.0\n",
      "48 0.47604308 0.0\n",
      "49 0.4959378 1.0\n",
      "50 0.49788934 1.0\n",
      "51 0.4968901 1.0\n",
      "52 0.49656162 1.0\n",
      "53 0.45983085 0.0\n",
      "54 0.49681437 1.0\n",
      "55 0.46078664 0.0\n",
      "56 0.446011 0.0\n",
      "57 0.4451268 0.0\n",
      "58 0.45063984 0.0\n",
      "59 0.49637118 1.0\n",
      "60 0.5002073 1.0\n",
      "61 0.45874366 0.0\n",
      "62 0.49997684 1.0\n",
      "63 0.49880463 1.0\n",
      "64 0.46381974 0.0\n",
      "65 0.50059843 1.0\n",
      "66 0.4544285 0.0\n",
      "67 0.45120412 0.0\n",
      "68 0.45904124 0.0\n",
      "69 0.48871246 0.0\n",
      "70 0.45507956 0.0\n",
      "71 0.49958742 1.0\n",
      "72 0.44909722 0.0\n",
      "73 0.4473057 0.0\n",
      "74 0.47564322 0.0\n",
      "75 0.47702205 0.0\n",
      "76 0.5012379 1.0\n",
      "77 0.49626714 1.0\n",
      "78 0.49779925 1.0\n",
      "79 0.49980077 1.0\n",
      "80 0.50077176 1.0\n",
      "81 0.4945269 1.0\n",
      "82 0.49930117 1.0\n",
      "83 0.4923813 1.0\n",
      "84 0.4974713 1.0\n",
      "85 0.49944037 1.0\n",
      "86 0.4863963 0.0\n",
      "87 0.49814057 1.0\n",
      "88 0.48917344 0.0\n",
      "89 0.46170416 0.0\n",
      "90 0.4973098 1.0\n",
      "91 0.50175226 1.0\n",
      "92 0.49608776 1.0\n",
      "93 0.44856554 0.0\n",
      "94 0.49894497 1.0\n",
      "95 0.4606224 0.0\n",
      "96 0.46075428 0.0\n",
      "97 0.4991433 1.0\n",
      "98 0.44573748 0.0\n",
      "99 0.49526772 1.0\n",
      "100 0.50080466 1.0\n",
      "101 0.46631438 0.0\n",
      "102 0.49904636 1.0\n",
      "103 0.4928017 1.0\n",
      "104 0.4757824 0.0\n",
      "105 0.4941628 1.0\n",
      "106 0.46251705 0.0\n",
      "107 0.43889666 0.0\n",
      "108 0.49650696 1.0\n",
      "109 0.49977252 1.0\n",
      "110 0.4596224 0.0\n",
      "111 0.50030553 1.0\n",
      "112 0.46239507 0.0\n",
      "113 0.46219984 0.0\n",
      "114 0.46395603 0.0\n",
      "115 0.4525724 0.0\n",
      "116 0.4979693 1.0\n",
      "117 0.4672475 0.0\n",
      "118 0.50263864 1.0\n",
      "119 0.4975972 1.0\n",
      "120 0.4394619 0.0\n",
      "121 0.49945882 1.0\n",
      "122 0.45360315 0.0\n",
      "123 0.4771718 0.0\n",
      "124 0.46579278 0.0\n",
      "125 0.49734926 1.0\n",
      "126 0.44732887 0.0\n",
      "127 0.49569118 1.0\n",
      "128 0.495467 1.0\n",
      "129 0.5000228 1.0\n",
      "130 0.44671556 0.0\n",
      "131 0.46023026 0.0\n",
      "132 0.4948102 1.0\n",
      "133 0.49962845 1.0\n",
      "134 0.50049466 1.0\n",
      "135 0.5009236 1.0\n",
      "136 0.47050154 0.0\n",
      "137 0.44268075 0.0\n",
      "138 0.49445143 1.0\n",
      "139 0.496768 1.0\n",
      "140 0.46355665 0.0\n",
      "141 0.49933285 1.0\n",
      "142 0.4994276 1.0\n",
      "143 0.48148662 0.0\n",
      "144 0.49568424 1.0\n",
      "145 0.5003048 1.0\n",
      "146 0.49246654 1.0\n",
      "147 0.45497477 0.0\n",
      "148 0.49321523 1.0\n",
      "149 0.480717 0.0\n",
      "150 0.46746892 0.0\n",
      "151 0.49948597 1.0\n",
      "152 0.45176533 0.0\n",
      "153 0.44534197 0.0\n",
      "154 0.49821588 1.0\n",
      "155 0.44015554 0.0\n",
      "156 0.46543026 0.0\n",
      "157 0.44662988 0.0\n",
      "158 0.5008809 1.0\n",
      "159 0.4975209 1.0\n",
      "160 0.49707583 1.0\n",
      "161 0.49890918 1.0\n",
      "162 0.49957573 1.0\n",
      "163 0.4559456 0.0\n",
      "164 0.50198764 1.0\n",
      "165 0.45774755 0.0\n",
      "166 0.4652906 0.0\n",
      "167 0.47579673 0.0\n",
      "168 0.48921418 0.0\n",
      "169 0.49757695 1.0\n",
      "170 0.49934885 1.0\n",
      "171 0.46476763 0.0\n",
      "172 0.50051564 1.0\n",
      "173 0.4925154 1.0\n",
      "174 0.4983805 1.0\n",
      "175 0.4956461 1.0\n",
      "176 0.4584747 0.0\n",
      "177 0.5007847 1.0\n",
      "178 0.49938843 1.0\n",
      "179 0.47345117 0.0\n",
      "180 0.4974933 1.0\n",
      "181 0.49909204 1.0\n",
      "182 0.4988873 1.0\n",
      "183 0.49187207 1.0\n",
      "184 0.43111116 0.0\n",
      "185 0.50052905 1.0\n",
      "186 0.50064373 1.0\n",
      "187 0.49700645 1.0\n",
      "188 0.46817228 0.0\n",
      "189 0.44936275 0.0\n",
      "190 0.45433885 0.0\n",
      "191 0.4671061 0.0\n",
      "192 0.49408454 1.0\n",
      "193 0.47699863 0.0\n",
      "194 0.49931186 1.0\n",
      "195 0.46625358 0.0\n",
      "196 0.4981446 1.0\n",
      "197 0.44977513 0.0\n",
      "198 0.45170054 0.0\n",
      "199 0.4961414 1.0\n",
      "200 0.46640453 0.0\n",
      "201 0.4975332 1.0\n",
      "202 0.5000628 1.0\n",
      "203 0.46021122 0.0\n",
      "204 0.49761662 1.0\n",
      "205 0.49579507 1.0\n",
      "206 0.45040518 0.0\n",
      "207 0.45837972 0.0\n",
      "208 0.452866 0.0\n",
      "209 0.4958257 1.0\n",
      "210 0.50065035 1.0\n",
      "211 0.49984154 1.0\n",
      "212 0.49630892 1.0\n",
      "213 0.49928886 1.0\n",
      "214 0.49991488 1.0\n",
      "215 0.43309554 0.0\n",
      "216 0.45137513 0.0\n",
      "217 0.4657471 0.0\n",
      "218 0.483078 0.0\n",
      "219 0.4763588 0.0\n",
      "220 0.44596294 0.0\n",
      "221 0.4438142 0.0\n",
      "222 0.46876132 0.0\n",
      "223 0.5026226 1.0\n",
      "224 0.49629498 1.0\n",
      "225 0.4820165 0.0\n",
      "226 0.5015128 1.0\n",
      "227 0.49953958 1.0\n",
      "228 0.46918133 0.0\n",
      "229 0.49869534 1.0\n",
      "230 0.4947518 1.0\n",
      "231 0.49968982 1.0\n",
      "232 0.49875957 1.0\n",
      "233 0.44676557 0.0\n",
      "234 0.4570159 0.0\n",
      "235 0.49974155 1.0\n",
      "236 0.44270584 0.0\n",
      "237 0.49388885 1.0\n",
      "238 0.45316446 0.0\n",
      "239 0.45523882 0.0\n",
      "240 0.49971017 1.0\n",
      "241 0.4553818 0.0\n",
      "242 0.45550615 0.0\n",
      "243 0.4410446 0.0\n",
      "244 0.4464411 0.0\n",
      "245 0.4998255 1.0\n",
      "246 0.49633333 1.0\n",
      "247 0.46849704 0.0\n",
      "248 0.4972862 1.0\n",
      "249 0.4976631 1.0\n",
      "250 0.4992193 1.0\n",
      "251 0.5013463 1.0\n",
      "252 0.46063435 0.0\n",
      "253 0.43675497 0.0\n",
      "254 0.46899414 0.0\n",
      "255 0.4447538 0.0\n",
      "256 0.49878404 1.0\n",
      "257 0.45786142 0.0\n",
      "258 0.50822824 1.0\n",
      "259 0.4978136 1.0\n",
      "260 0.45986268 0.0\n",
      "261 0.5005946 1.0\n",
      "262 0.45208457 0.0\n",
      "263 0.49739203 1.0\n",
      "264 0.45826638 0.0\n",
      "265 0.44369632 0.0\n",
      "266 0.43540633 0.0\n",
      "267 0.4940421 1.0\n",
      "268 0.5005003 1.0\n",
      "269 0.4486035 0.0\n",
      "270 0.47144982 0.0\n",
      "271 0.49994943 1.0\n",
      "272 0.49741215 1.0\n",
      "273 0.43461904 0.0\n",
      "274 0.4562014 0.0\n",
      "275 0.49977177 1.0\n",
      "276 0.49588507 1.0\n",
      "277 0.48484167 0.0\n",
      "278 0.50140774 1.0\n",
      "279 0.44702536 0.0\n",
      "280 0.4999612 1.0\n",
      "281 0.4600079 0.0\n",
      "282 0.4593683 0.0\n",
      "283 0.49738806 1.0\n",
      "284 0.4635581 0.0\n",
      "285 0.49358013 1.0\n",
      "286 0.45045558 0.0\n",
      "287 0.4605999 0.0\n",
      "288 0.4568029 0.0\n",
      "289 0.42382616 0.0\n",
      "290 0.5001508 1.0\n",
      "291 0.46977627 0.0\n",
      "292 0.49748558 1.0\n",
      "293 0.4982986 1.0\n",
      "294 0.4926025 1.0\n",
      "295 0.4633471 0.0\n",
      "296 0.50056016 1.0\n",
      "297 0.47855115 0.0\n",
      "298 0.46671456 0.0\n",
      "299 0.49936953 1.0\n",
      "300 0.44912195 0.0\n",
      "301 0.44976288 0.0\n",
      "302 0.5009297 1.0\n",
      "303 0.4820681 0.0\n",
      "304 0.46215686 0.0\n",
      "305 0.49656436 1.0\n",
      "306 0.44895867 0.0\n",
      "307 0.5020689 1.0\n",
      "308 0.4630179 0.0\n",
      "309 0.48867068 0.0\n",
      "310 0.49782458 1.0\n",
      "311 0.5001587 1.0\n",
      "312 0.49623537 1.0\n",
      "313 0.4971135 1.0\n",
      "314 0.5010142 1.0\n",
      "315 0.49337724 1.0\n",
      "316 0.44441572 0.0\n",
      "317 0.49777645 1.0\n",
      "318 0.49720785 1.0\n",
      "319 0.49736997 1.0\n",
      "320 0.46939147 0.0\n",
      "321 0.47818533 0.0\n",
      "322 0.48885077 0.0\n",
      "323 0.49809718 1.0\n",
      "324 0.4532162 0.0\n",
      "325 0.45295677 0.0\n",
      "326 0.45356795 0.0\n",
      "327 0.49709502 1.0\n",
      "328 0.44015968 0.0\n",
      "329 0.49912092 1.0\n",
      "330 0.49737406 1.0\n",
      "331 0.49924996 1.0\n",
      "332 0.46581623 0.0\n",
      "333 0.45497385 0.0\n",
      "334 0.4603591 0.0\n",
      "335 0.48315093 0.0\n",
      "336 0.49761516 1.0\n",
      "337 0.49819714 1.0\n",
      "338 0.44448122 0.0\n",
      "339 0.5001979 1.0\n",
      "340 0.4971763 1.0\n",
      "341 0.48135474 0.0\n",
      "342 0.50004524 1.0\n",
      "343 0.49479482 1.0\n",
      "344 0.4739641 0.0\n",
      "345 0.4659266 0.0\n",
      "346 0.45620847 0.0\n",
      "347 0.4497865 0.0\n",
      "348 0.4993051 1.0\n",
      "349 0.49492317 1.0\n",
      "350 0.46846873 0.0\n",
      "351 0.44331953 0.0\n",
      "352 0.45833734 0.0\n",
      "353 0.499366 1.0\n",
      "354 0.49423277 1.0\n",
      "355 0.42880628 0.0\n",
      "356 0.49365723 1.0\n",
      "357 0.49786744 1.0\n",
      "358 0.49852377 1.0\n",
      "359 0.45466357 0.0\n",
      "360 0.49703395 1.0\n",
      "361 0.4982123 1.0\n",
      "362 0.45127848 0.0\n",
      "363 0.49711847 1.0\n",
      "364 0.4780594 0.0\n",
      "365 0.52286124 1.0\n",
      "366 0.49851364 1.0\n",
      "367 0.49966133 1.0\n",
      "368 0.49905413 1.0\n",
      "369 0.49650383 1.0\n",
      "370 0.46747226 0.0\n",
      "371 0.4637283 0.0\n",
      "372 0.44652987 0.0\n",
      "373 0.45711786 0.0\n",
      "374 0.47626358 0.0\n",
      "375 0.49809575 1.0\n",
      "376 0.4978383 1.0\n",
      "377 0.50034255 1.0\n",
      "378 0.4964683 1.0\n",
      "379 0.44613203 0.0\n",
      "380 0.46251512 0.0\n",
      "381 0.4773826 0.0\n",
      "382 0.49909374 1.0\n",
      "383 0.50171125 1.0\n",
      "384 0.49710107 1.0\n",
      "385 0.49545366 1.0\n",
      "386 0.457458 0.0\n",
      "387 0.44678137 0.0\n",
      "388 0.49642465 1.0\n",
      "389 0.48944044 0.0\n",
      "390 0.5009397 1.0\n",
      "391 0.496971 1.0\n",
      "392 0.47588035 0.0\n",
      "393 0.46845448 0.0\n",
      "394 0.46137094 0.0\n",
      "395 0.47970074 0.0\n",
      "396 0.49592718 1.0\n",
      "397 0.49642706 1.0\n",
      "398 0.46148407 0.0\n",
      "399 0.44954932 0.0\n",
      "400 0.50278735 1.0\n",
      "401 0.48880076 0.0\n",
      "402 0.49723238 1.0\n",
      "403 0.4967411 1.0\n",
      "404 0.48421708 0.0\n",
      "405 0.4991888 1.0\n",
      "406 0.49782377 1.0\n",
      "407 0.44860822 0.0\n",
      "408 0.44270343 0.0\n",
      "409 0.4977586 1.0\n",
      "410 0.4790251 0.0\n",
      "411 0.5022505 1.0\n",
      "412 0.47829542 0.0\n",
      "413 0.43125868 0.0\n",
      "414 0.458718 0.0\n",
      "415 0.46334723 0.0\n",
      "416 0.49155855 0.0\n",
      "417 0.49253926 1.0\n",
      "418 0.4658603 0.0\n",
      "419 0.45242503 0.0\n",
      "420 0.49897155 1.0\n",
      "421 0.4628935 0.0\n",
      "422 0.44410476 0.0\n",
      "423 0.4987842 1.0\n",
      "424 0.49771255 1.0\n",
      "425 0.4647851 0.0\n",
      "426 0.49778536 1.0\n",
      "427 0.4703363 0.0\n",
      "428 0.4655313 0.0\n",
      "429 0.50118035 1.0\n",
      "430 0.49578598 1.0\n",
      "431 0.45846596 0.0\n",
      "432 0.49703592 1.0\n",
      "433 0.44850355 0.0\n",
      "434 0.4612306 0.0\n",
      "435 0.49396178 1.0\n",
      "436 0.4659206 0.0\n",
      "437 0.49592233 1.0\n",
      "438 0.45997173 0.0\n",
      "439 0.5014867 1.0\n",
      "440 0.49805674 1.0\n",
      "441 0.4841595 0.0\n",
      "442 0.49803612 1.0\n",
      "443 0.4818061 0.0\n",
      "444 0.44231063 0.0\n",
      "445 0.49755353 1.0\n",
      "446 0.49922514 1.0\n",
      "447 0.50185364 1.0\n",
      "448 0.44920152 0.0\n",
      "449 0.49965823 1.0\n",
      "450 0.50038016 1.0\n",
      "451 0.43174213 0.0\n",
      "452 0.50052655 1.0\n",
      "453 0.49574775 1.0\n",
      "454 0.49817017 1.0\n",
      "455 0.4998072 1.0\n",
      "456 0.49940333 1.0\n",
      "457 0.50555855 0.0\n",
      "458 0.49746206 1.0\n",
      "459 0.4539297 0.0\n",
      "460 0.49878603 1.0\n",
      "461 0.45768845 0.0\n",
      "462 0.4807482 0.0\n",
      "463 0.49610078 1.0\n",
      "464 0.4661051 0.0\n",
      "465 0.47103637 0.0\n",
      "466 0.43040773 0.0\n",
      "467 0.46304956 0.0\n",
      "468 0.4778372 0.0\n",
      "469 0.4964356 1.0\n",
      "470 0.4511582 0.0\n",
      "471 0.4483091 0.0\n",
      "472 0.4997725 1.0\n",
      "473 0.49829817 1.0\n",
      "474 0.47547087 0.0\n",
      "475 0.49710461 1.0\n",
      "476 0.49662238 1.0\n",
      "477 0.49559978 1.0\n",
      "478 0.49764627 1.0\n",
      "479 0.50159377 1.0\n",
      "480 0.45700136 0.0\n",
      "481 0.4631542 0.0\n",
      "482 0.47016144 0.0\n",
      "483 0.4608445 0.0\n",
      "484 0.49684092 1.0\n",
      "485 0.50092036 1.0\n",
      "486 0.49990034 1.0\n",
      "487 0.49567527 1.0\n",
      "488 0.4595416 0.0\n",
      "489 0.49673787 1.0\n",
      "490 0.49394596 1.0\n",
      "491 0.49570394 1.0\n",
      "492 0.46496555 0.0\n",
      "493 0.49616328 1.0\n",
      "494 0.5022052 1.0\n",
      "495 0.49929848 1.0\n",
      "496 0.49633637 1.0\n",
      "497 0.44605708 0.0\n",
      "498 0.4964826 1.0\n",
      "499 0.46181184 0.0\n",
      "500 0.4667592 0.0\n",
      "501 0.49819106 1.0\n",
      "502 0.49997404 1.0\n",
      "503 0.4627682 0.0\n",
      "504 0.47613424 0.0\n",
      "505 0.49860957 1.0\n",
      "506 0.438369 0.0\n",
      "507 0.4967899 1.0\n",
      "508 0.48637724 0.0\n",
      "509 0.49900374 1.0\n",
      "510 0.5009142 1.0\n",
      "511 0.45921516 0.0\n",
      "512 0.5081074 1.0\n",
      "513 0.46950486 0.0\n",
      "514 0.4517608 0.0\n",
      "515 0.49646178 1.0\n",
      "516 0.4634456 0.0\n",
      "517 0.51375985 1.0\n",
      "518 0.49665076 1.0\n",
      "519 0.50147414 1.0\n",
      "520 0.4774935 0.0\n",
      "521 0.44948977 0.0\n",
      "522 0.49796438 1.0\n",
      "523 0.5001903 1.0\n",
      "524 0.45300686 0.0\n",
      "525 0.44281253 0.0\n",
      "526 0.44228902 0.0\n",
      "527 0.49257696 1.0\n",
      "528 0.49404562 1.0\n",
      "529 0.49846062 1.0\n",
      "530 0.45440915 0.0\n",
      "531 0.44782412 0.0\n",
      "532 0.4943325 1.0\n",
      "533 0.4422395 0.0\n",
      "534 0.45086738 0.0\n",
      "535 0.49941322 1.0\n",
      "536 0.49475595 1.0\n",
      "537 0.500162 1.0\n",
      "538 0.4978201 1.0\n",
      "539 0.4201926 0.0\n",
      "540 0.5003892 1.0\n",
      "541 0.49902433 1.0\n",
      "542 0.45348886 0.0\n",
      "543 0.49658 1.0\n",
      "544 0.50623405 1.0\n",
      "545 0.47636098 0.0\n",
      "546 0.4973806 1.0\n",
      "547 0.47650763 0.0\n",
      "548 0.4664661 0.0\n",
      "549 0.44859192 0.0\n",
      "550 0.4648277 0.0\n",
      "551 0.4997113 1.0\n",
      "552 0.49895385 1.0\n",
      "553 0.4647932 0.0\n",
      "554 0.49879628 1.0\n",
      "555 0.5001387 1.0\n",
      "556 0.44888663 0.0\n",
      "557 0.4978448 1.0\n",
      "558 0.47294167 0.0\n",
      "559 0.5056831 1.0\n",
      "560 0.46254274 0.0\n",
      "561 0.4549709 0.0\n",
      "562 0.4372408 0.0\n",
      "563 0.5014207 1.0\n",
      "564 0.46490052 0.0\n",
      "565 0.496756 1.0\n",
      "566 0.4996679 1.0\n",
      "567 0.45229232 0.0\n",
      "568 0.44852433 0.0\n",
      "569 0.50055975 1.0\n",
      "570 0.48893103 0.0\n",
      "571 0.46131152 0.0\n",
      "572 0.4621392 0.0\n",
      "573 0.49726674 1.0\n",
      "574 0.50090986 1.0\n",
      "575 0.49689445 1.0\n",
      "576 0.4957775 1.0\n",
      "577 0.5001317 1.0\n",
      "578 0.45646465 0.0\n",
      "579 0.49920446 1.0\n",
      "580 0.49328715 1.0\n",
      "581 0.4738394 0.0\n",
      "582 0.5000031 1.0\n",
      "583 0.49664003 1.0\n",
      "584 0.5012972 1.0\n",
      "585 0.45801383 0.0\n",
      "586 0.49569163 1.0\n",
      "587 0.4564189 0.0\n",
      "588 0.5001883 1.0\n",
      "589 0.49938264 1.0\n",
      "590 0.4561397 0.0\n",
      "591 0.49737763 1.0\n",
      "592 0.502988 1.0\n",
      "593 0.49777716 1.0\n",
      "594 0.45483822 0.0\n",
      "595 0.4956484 1.0\n",
      "596 0.4979951 1.0\n",
      "597 0.49338406 1.0\n",
      "598 0.5006022 1.0\n",
      "599 0.4983803 1.0\n",
      "600 0.4560628 0.0\n",
      "601 0.45678717 0.0\n",
      "602 0.5002171 1.0\n",
      "603 0.49351713 1.0\n",
      "604 0.4995108 1.0\n",
      "605 0.4937927 1.0\n",
      "606 0.5011215 1.0\n",
      "607 0.4971614 1.0\n",
      "608 0.44245574 0.0\n",
      "609 0.49640107 1.0\n",
      "610 0.49981153 1.0\n",
      "611 0.49702233 1.0\n",
      "612 0.50151676 1.0\n",
      "613 0.46879503 0.0\n",
      "614 0.45532456 0.0\n",
      "615 0.5008424 1.0\n",
      "616 0.50259906 1.0\n",
      "617 0.45787466 0.0\n",
      "618 0.46084833 0.0\n",
      "619 0.41098607 0.0\n",
      "620 0.44662926 0.0\n",
      "621 0.5000725 1.0\n",
      "622 0.46115294 0.0\n",
      "623 0.45087886 0.0\n",
      "624 0.4990324 1.0\n",
      "625 0.47941154 0.0\n",
      "626 0.49786773 1.0\n",
      "627 0.49697557 1.0\n",
      "628 0.49831474 1.0\n",
      "629 0.46072662 0.0\n",
      "630 0.49631393 1.0\n",
      "631 0.51111746 1.0\n",
      "632 0.49586138 1.0\n",
      "633 0.5070826 1.0\n",
      "634 0.49866825 1.0\n",
      "635 0.44449162 0.0\n",
      "636 0.48068267 0.0\n",
      "637 0.48029718 0.0\n",
      "638 0.47459856 0.0\n",
      "639 0.47389555 0.0\n",
      "640 0.45530385 0.0\n",
      "641 0.50026226 1.0\n",
      "642 0.50114065 1.0\n",
      "643 0.44624144 0.0\n",
      "644 0.50161403 1.0\n",
      "645 0.44293195 0.0\n",
      "646 0.44913548 0.0\n",
      "647 0.46733004 0.0\n",
      "648 0.46408802 0.0\n",
      "649 0.472247 0.0\n",
      "650 0.4941149 1.0\n",
      "651 0.46523952 0.0\n",
      "652 0.4296823 0.0\n",
      "653 0.5009198 1.0\n",
      "654 0.44945794 0.0\n",
      "655 0.4947686 1.0\n",
      "656 0.47640735 0.0\n",
      "657 0.49742657 1.0\n",
      "658 0.49726528 1.0\n",
      "659 0.4570542 0.0\n",
      "660 0.47160655 0.0\n",
      "661 0.50303423 1.0\n",
      "662 0.49697733 1.0\n",
      "663 0.4577968 0.0\n",
      "664 0.49679053 1.0\n",
      "665 0.4584406 0.0\n",
      "666 0.45719486 0.0\n",
      "667 0.4985381 1.0\n",
      "668 0.49927986 1.0\n",
      "669 0.41993743 0.0\n",
      "670 0.47271788 0.0\n",
      "671 0.49762815 1.0\n",
      "672 0.46244383 0.0\n",
      "673 0.47035807 0.0\n",
      "674 0.4263187 0.0\n",
      "675 0.5003257 1.0\n",
      "676 0.49757108 1.0\n",
      "677 0.49760932 1.0\n",
      "678 0.46319178 0.0\n",
      "679 0.45718795 0.0\n",
      "680 0.5009363 1.0\n",
      "681 0.47087187 0.0\n",
      "682 0.4936118 1.0\n",
      "683 0.49824148 1.0\n",
      "684 0.45968285 0.0\n",
      "685 0.44827574 0.0\n",
      "686 0.4603781 0.0\n",
      "687 0.48663932 0.0\n",
      "688 0.49755785 1.0\n",
      "689 0.4581133 0.0\n",
      "690 0.45546022 0.0\n",
      "691 0.5008212 1.0\n",
      "692 0.5001917 1.0\n",
      "693 0.4957626 1.0\n",
      "694 0.48549348 0.0\n",
      "695 0.49819335 1.0\n",
      "696 0.49864385 1.0\n",
      "697 0.45433524 0.0\n",
      "698 0.4993904 1.0\n",
      "699 0.4636568 0.0\n",
      "700 0.50082153 1.0\n",
      "701 0.46874148 0.0\n",
      "702 0.49486122 1.0\n",
      "703 0.49532175 1.0\n",
      "704 0.4974161 1.0\n",
      "705 0.49965885 1.0\n",
      "706 0.5018752 1.0\n",
      "707 0.4710955 0.0\n",
      "708 0.5015856 1.0\n",
      "709 0.49908873 1.0\n",
      "710 0.44811293 0.0\n",
      "711 0.49633408 1.0\n",
      "712 0.4981651 1.0\n",
      "713 0.49602538 1.0\n",
      "714 0.46200138 0.0\n",
      "715 0.49978548 1.0\n",
      "716 0.46003437 0.0\n",
      "717 0.4978289 1.0\n",
      "718 0.45456916 0.0\n",
      "719 0.49723384 1.0\n",
      "720 0.45746428 0.0\n",
      "721 0.45962715 0.0\n",
      "722 0.49742162 1.0\n",
      "723 0.46375775 0.0\n",
      "724 0.44644767 0.0\n",
      "725 0.502212 1.0\n",
      "726 0.44256538 0.0\n",
      "727 0.50188106 1.0\n",
      "728 0.4577514 0.0\n",
      "729 0.49730375 1.0\n",
      "730 0.46853107 0.0\n",
      "731 0.46784842 0.0\n",
      "732 0.45046267 0.0\n",
      "733 0.49651685 1.0\n",
      "734 0.4888217 0.0\n",
      "735 0.4964586 1.0\n",
      "736 0.46724755 0.0\n",
      "737 0.45259318 0.0\n",
      "738 0.49772218 1.0\n",
      "739 0.4548391 0.0\n",
      "740 0.49742264 1.0\n",
      "741 0.5026009 1.0\n",
      "742 0.45023796 0.0\n",
      "743 0.49872962 1.0\n",
      "744 0.47522247 0.0\n",
      "745 0.46290466 0.0\n",
      "746 0.44739076 0.0\n",
      "747 0.46414065 0.0\n",
      "748 0.46671182 0.0\n",
      "749 0.4967983 1.0\n",
      "750 0.45340344 0.0\n",
      "751 0.49487472 1.0\n",
      "752 0.49752453 1.0\n",
      "753 0.46061495 0.0\n",
      "754 0.49502108 1.0\n",
      "755 0.42345434 0.0\n",
      "756 0.5002999 1.0\n",
      "757 0.49618828 1.0\n",
      "758 0.450416 0.0\n",
      "759 0.49778914 1.0\n",
      "760 0.44359997 0.0\n",
      "761 0.5006967 1.0\n",
      "762 0.4535665 0.0\n",
      "763 0.48834792 0.0\n",
      "764 0.49532455 1.0\n",
      "765 0.50472426 1.0\n",
      "766 0.49361512 1.0\n",
      "767 0.45231903 0.0\n",
      "768 0.500229 1.0\n",
      "769 0.49861586 1.0\n",
      "770 0.4575978 0.0\n",
      "771 0.49696013 1.0\n",
      "772 0.4962027 1.0\n",
      "773 0.5028007 1.0\n",
      "774 0.50001496 1.0\n",
      "775 0.45904505 0.0\n",
      "776 0.44159672 0.0\n",
      "777 0.48884737 0.0\n",
      "778 0.45136482 0.0\n",
      "779 0.49840397 1.0\n",
      "780 0.48156324 0.0\n",
      "781 0.49929577 1.0\n",
      "782 0.4989982 1.0\n",
      "783 0.5129782 1.0\n",
      "784 0.470767 0.0\n",
      "785 0.4970287 1.0\n",
      "786 0.46709323 0.0\n",
      "787 0.5021501 1.0\n",
      "788 0.4999479 1.0\n",
      "789 0.45176297 0.0\n",
      "790 0.4983943 1.0\n",
      "791 0.47838837 0.0\n",
      "792 0.49847597 1.0\n",
      "793 0.49978304 1.0\n",
      "794 0.49747092 1.0\n",
      "795 0.4646674 0.0\n",
      "796 0.47432148 0.0\n",
      "797 0.4830331 0.0\n",
      "798 0.472736 0.0\n",
      "799 0.5004225 1.0\n",
      "800 0.47354123 0.0\n",
      "801 0.4694133 0.0\n",
      "802 0.4994325 1.0\n",
      "803 0.4491006 0.0\n",
      "804 0.49738267 1.0\n",
      "805 0.4387151 0.0\n",
      "806 0.4874857 0.0\n",
      "807 0.44894597 0.0\n",
      "808 0.4572726 0.0\n",
      "809 0.45324886 0.0\n",
      "810 0.49753144 1.0\n",
      "811 0.49781805 1.0\n",
      "812 0.45887005 0.0\n",
      "813 0.49785367 1.0\n",
      "814 0.50030315 1.0\n",
      "815 0.49796978 1.0\n",
      "816 0.49706313 1.0\n",
      "817 0.44592127 0.0\n",
      "818 0.4632935 0.0\n",
      "819 0.4410916 0.0\n",
      "820 0.4979873 1.0\n",
      "821 0.50145024 1.0\n",
      "822 0.49207866 1.0\n",
      "823 0.45810285 0.0\n",
      "824 0.448971 0.0\n",
      "825 0.45418566 0.0\n",
      "826 0.4998858 1.0\n",
      "827 0.46414834 0.0\n",
      "828 0.49787968 1.0\n",
      "829 0.49892983 1.0\n",
      "830 0.50204664 1.0\n",
      "831 0.4785188 0.0\n",
      "832 0.49695614 1.0\n",
      "833 0.5004311 1.0\n",
      "834 0.4952781 1.0\n",
      "835 0.45104492 0.0\n",
      "836 0.49965465 1.0\n",
      "837 0.454728 0.0\n",
      "838 0.46104902 0.0\n",
      "839 0.4508762 0.0\n",
      "840 0.49961588 1.0\n",
      "841 0.47918877 0.0\n",
      "842 0.49912968 1.0\n",
      "843 0.46293646 0.0\n",
      "844 0.47478124 0.0\n",
      "845 0.45087668 0.0\n",
      "846 0.49597102 1.0\n",
      "847 0.48393378 0.0\n",
      "848 0.47717404 0.0\n",
      "849 0.50163895 1.0\n",
      "850 0.4666626 0.0\n",
      "851 0.44919902 0.0\n",
      "852 0.45376506 0.0\n",
      "853 0.5012524 1.0\n",
      "854 0.4774204 0.0\n",
      "855 0.46008962 0.0\n",
      "856 0.5031589 1.0\n",
      "857 0.47819105 0.0\n",
      "858 0.49753344 1.0\n",
      "859 0.45116913 0.0\n",
      "860 0.4983521 1.0\n",
      "861 0.42861342 0.0\n",
      "862 0.49285603 1.0\n",
      "863 0.45428142 0.0\n",
      "864 0.5003359 1.0\n",
      "865 0.43831536 0.0\n",
      "866 0.499458 1.0\n",
      "867 0.500438 1.0\n",
      "868 0.47192156 0.0\n",
      "869 0.47039452 0.0\n",
      "870 0.4628042 0.0\n",
      "871 0.49984562 1.0\n",
      "872 0.49675167 1.0\n",
      "873 0.49421123 1.0\n",
      "874 0.49597365 1.0\n",
      "875 0.4992803 1.0\n",
      "876 0.49607134 1.0\n",
      "877 0.49857903 1.0\n",
      "878 0.49671832 1.0\n",
      "879 0.4993632 1.0\n",
      "880 0.51160777 1.0\n",
      "881 0.5002341 1.0\n",
      "882 0.4959269 1.0\n",
      "883 0.49525255 1.0\n",
      "884 0.5006029 1.0\n",
      "885 0.45993254 0.0\n",
      "886 0.5007713 1.0\n",
      "887 0.49639198 1.0\n",
      "888 0.44088894 0.0\n",
      "889 0.46017328 0.0\n",
      "890 0.46077123 0.0\n",
      "891 0.49944183 1.0\n",
      "892 0.47428697 0.0\n",
      "893 0.49695352 1.0\n",
      "894 0.49955535 1.0\n",
      "895 0.5015531 1.0\n",
      "896 0.49633396 1.0\n",
      "897 0.45450434 0.0\n",
      "898 0.4510218 0.0\n",
      "899 0.4457594 0.0\n",
      "900 0.47679853 0.0\n",
      "901 0.45691538 0.0\n",
      "902 0.49913305 1.0\n",
      "903 0.49942917 1.0\n",
      "904 0.49478713 1.0\n",
      "905 0.49688008 1.0\n",
      "906 0.45052978 0.0\n",
      "907 0.49939302 1.0\n",
      "908 0.4983021 1.0\n",
      "909 0.47035292 0.0\n",
      "910 0.467274 0.0\n",
      "911 0.45054442 0.0\n",
      "912 0.4370844 0.0\n",
      "913 0.50012416 1.0\n",
      "914 0.4754557 0.0\n",
      "915 0.50030446 1.0\n",
      "916 0.44735292 0.0\n",
      "917 0.4602855 0.0\n",
      "918 0.46563083 0.0\n",
      "919 0.42837012 0.0\n",
      "920 0.44770652 0.0\n",
      "921 0.46293485 0.0\n",
      "922 0.5000357 1.0\n",
      "923 0.4986847 1.0\n",
      "924 0.501734 1.0\n",
      "925 0.5000887 1.0\n",
      "926 0.4980654 1.0\n",
      "927 0.4970276 1.0\n",
      "928 0.5015247 1.0\n",
      "929 0.49952006 1.0\n",
      "930 0.478188 0.0\n",
      "931 0.49655515 1.0\n",
      "932 0.45518762 0.0\n",
      "933 0.49996337 1.0\n",
      "934 0.49884808 1.0\n",
      "935 0.4500815 0.0\n",
      "936 0.49586046 1.0\n",
      "937 0.49485737 1.0\n",
      "938 0.4511098 0.0\n",
      "939 0.4952177 1.0\n",
      "940 0.496412 1.0\n",
      "941 0.49728447 1.0\n",
      "942 0.4685175 0.0\n",
      "943 0.45994753 0.0\n",
      "944 0.45690647 0.0\n",
      "945 0.5001498 1.0\n",
      "946 0.4405189 0.0\n",
      "947 0.49974415 1.0\n",
      "948 0.48326403 0.0\n",
      "949 0.50092125 1.0\n",
      "950 0.501566 1.0\n",
      "951 0.4980708 1.0\n",
      "952 0.4954123 1.0\n",
      "953 0.4474221 0.0\n",
      "954 0.4416287 0.0\n",
      "955 0.49785632 1.0\n",
      "956 0.49424925 1.0\n",
      "957 0.4536147 0.0\n",
      "958 0.49856257 1.0\n",
      "959 0.49843034 1.0\n",
      "960 0.49782345 1.0\n",
      "961 0.4976147 1.0\n",
      "962 0.50130993 1.0\n",
      "963 0.4559363 0.0\n",
      "964 0.45159608 0.0\n",
      "965 0.46552998 0.0\n",
      "966 0.50114715 1.0\n",
      "967 0.49913445 1.0\n",
      "968 0.47724366 0.0\n",
      "969 0.4549836 0.0\n",
      "970 0.4826448 0.0\n",
      "971 0.5013571 1.0\n",
      "972 0.5000106 1.0\n",
      "973 0.4606672 0.0\n",
      "974 0.47808546 0.0\n",
      "975 0.49929988 1.0\n",
      "976 0.50130874 1.0\n",
      "977 0.46686316 0.0\n",
      "978 0.4998417 1.0\n",
      "979 0.49870896 1.0\n",
      "980 0.49564546 1.0\n",
      "981 0.47695133 0.0\n",
      "982 0.50285095 1.0\n",
      "983 0.4539976 0.0\n",
      "984 0.49767485 1.0\n",
      "985 0.47398376 0.0\n",
      "986 0.5062609 1.0\n",
      "987 0.4981962 1.0\n",
      "988 0.46370047 0.0\n",
      "989 0.45957685 0.0\n",
      "990 0.4975596 1.0\n",
      "991 0.49909374 1.0\n",
      "992 0.49638838 1.0\n",
      "993 0.49878037 1.0\n",
      "994 0.45490617 0.0\n",
      "995 0.44397396 0.0\n",
      "996 0.45781544 0.0\n",
      "997 0.4856417 0.0\n",
      "998 0.4570793 0.0\n",
      "999 0.4985994 1.0\n",
      "1000 0.4973519 1.0\n",
      "1001 0.4976292 1.0\n",
      "1002 0.47990745 0.0\n",
      "1003 0.46624398 0.0\n",
      "1004 0.431797 0.0\n",
      "1005 0.5000079 1.0\n",
      "1006 0.46825746 0.0\n",
      "1007 0.49721622 1.0\n",
      "1008 0.4539578 0.0\n",
      "1009 0.4990835 1.0\n",
      "1010 0.45008117 0.0\n",
      "1011 0.50053114 1.0\n",
      "1012 0.4468362 0.0\n",
      "1013 0.5031881 1.0\n",
      "1014 0.49941435 1.0\n",
      "1015 0.48864588 0.0\n",
      "1016 0.49412286 1.0\n",
      "1017 0.4598774 0.0\n",
      "1018 0.4972032 1.0\n",
      "1019 0.4946941 1.0\n",
      "1020 0.49587926 1.0\n"
     ]
    }
   ],
   "source": [
    "path_file = '/Users/Kamyab/Documents/UoN/MLiS_2/Project/Data/test_data/test_data/'\n",
    "test_names = np.arange(1020) + 1\n",
    "with open(path_file + 'submission.csv', mode='w', newline='') as submission:\n",
    "    writer = csv.writer(submission, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['image_id', 'angle', 'speed'])\n",
    "    for name in test_names:     \n",
    "        img = cv2.imread(path_file + str(name) + '.png')\n",
    "#         print(img)\n",
    "        prediction = compute_steering_angle(model, img)\n",
    "        if prediction[1] < 0.5:\n",
    "            prediction[1] = 0\n",
    "        else:\n",
    "            prediction[1] = 1\n",
    "        writer.writerow([name, prediction[0], prediction[1]])\n",
    "        print(name, prediction[0], prediction[1])\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
